#pragma once

//https://github.com/stevenlovegrove/Pangolin/issues/352
#ifdef max
#undef max
#endif

#ifdef min
#undef min
#endif

#include <vector>
#include <array>
#include <cmath>
#include <math.h>
#include <algorithm>
#include <numeric>
#include <functional>
#include <random>
#include <fstream>

#include "tools/profiling.h"

namespace tools
{
	namespace math
	{
		// https://stackoverflow.com/questions/24518989/how-to-perform-1-dimensional-valid-convolution
	// The conv function implements the standard convolution method,
	// where the output size is the sum of input sizes minus one.
		template<typename T>
		std::vector<T> conv(std::vector<T> const& f, std::vector<T> const& g)
		{
			int const nf = f.size();
			int const ng = g.size();
			int const n = nf + ng - 1;
			std::vector<T> out(n, T());
			for (auto i(0); i < n; ++i) {
				int const jmn = (i >= ng - 1) ? i - (ng - 1) : 0;
				int const jmx = (i < nf - 1) ? i : nf - 1;
				for (auto j(jmn); j <= jmx; ++j) {
					out[i] += (f[j] * g[i - j]);
				}
			}
			return out;
		}

		// conv performs standard convolution, while conv_valid performs valid convolution,
		// which is generally more efficient for some applications where zero-padded areas
		// are not desired in the output.

		// https://stackoverflow.com/questions/24518989/how-to-perform-1-dimensional-valid-convolution
		// The conv_valid function implements the valid convolution method,
		// where the output size is the absolute difference between the input sizes plus one.
		template<typename T>
		std::vector<T> conv_valid(std::vector<T> const& f, std::vector<T> const& g)
		{
			int const nf = f.size();
			int const ng = g.size();
			std::vector<T> const& min_v = (nf < ng) ? f : g;
			std::vector<T> const& max_v = (nf < ng) ? g : f;
			int const n = std::max(nf, ng) - std::min(nf, ng) + 1;
			std::vector<T> out(n, T());
			for (auto i(0); i < n; ++i) {
				for (int j(min_v.size() - 1), k(i); j >= 0; --j) {
					out[i] += min_v[j] * max_v[k];
					++k;
				}
			}
			return out;
		}

		template<typename T>
		std::vector<T> gaussNorm(const std::vector<int>& rangeX, const T& position, const T& sigma)
		{
			std::vector<T> g(rangeX.size());
			for (int i = 0; i < g.size(); i++)
				g[i] = exp(-0.5 * pow((rangeX[i] - position), 2) / pow(sigma, 2));

			if (!g.empty())
			{
				double sumOfG = std::reduce(g.begin(), g.end());
				for (int i = 0; i < g.size(); i++)
					g[i] = g[i] / sumOfG;
			}

			return g;
		}

		template<typename T>
		std::vector<T> gauss(const std::vector<int>& rangeX, const T& position, const T& sigma)
		{
			std::vector<T> g(rangeX.size());

			for (int i = 0; i < g.size(); i++)
				g[i] = exp(-0.5 * pow((rangeX[i] - position), 2) / pow(sigma, 2));

			return g;
		}

		template<typename T>
		std::vector<T> circularGauss(uint32_t size, const T& sigma, const T& position)
		{
			uint32_t l = size - 2 * 1 + 2;
			uint32_t m = 1;
			T r = position - static_cast<T>(m); // Calculate the shift with fractional part
			T rem = std::fmod(r, static_cast<T>(l)); // Calculate the remainder
			T positionShifted = rem + static_cast<T>(m); // Apply the shifted position

			std::vector<T> g(size);
			std::vector<T> xRange(size);
			std::iota(xRange.begin(), xRange.end(), static_cast<T>(1));

			std::vector<T> d(size);
			std::vector<T> lMinusd(size);
			std::transform(xRange.begin(), xRange.end(), d.begin(), [&positionShifted, &l](T element) { return std::abs(element - positionShifted); });
			std::transform(d.begin(), d.end(), lMinusd.begin(), [&l](T element) { return -1 * (element - l); });
			for (int i = 0; i < size; i++)
			{
				g[i] = std::exp(-0.5 * std::pow(std::min(d[i], lMinusd[i]), 2) / std::pow(sigma, 2));
			}

			return g;
		}

		template<typename T>
		std::vector<T> obtainCircularVector(const std::vector<int>& indices, const std::vector<T>& contents)
		{
			std::vector<T> newContents(indices.size());
			for (int i = 0; i < indices.size(); i++)
				newContents[i] = contents[indices[i] - 1];
			return newContents;
		}

		template<typename T>
		std::vector<T> sigmoid(const std::vector<T>& x, T beta, T x0)
		{
			std::vector<T> s(x.size());
			for (int i = 0; i < static_cast<int>(s.size()); i++)
				s[i] = 1 / (1 + exp(-beta * (x[i] - x0)));
			return s;
		}

		template<typename T>
		std::vector<T> heaviside(const std::vector<T>& x, T threshold)
		{
			std::vector<T> h(x.size());
			for (int i = 0; i < static_cast<int>(h.size()); i++)
				h[i] = (x[i] > threshold) ? 1 : 0;
			return h;
		}

		template<typename T>
		std::vector<T> sumGauss(const std::vector<T>& gauss1, const std::vector<T>& gauss2)
		{
			std::vector<T> gaussResult(gauss1.size());
			for (int i = 0; i < gauss1.size(); i++)
				gaussResult[i] = gauss1[i] + gauss2[i];
			return gaussResult;
		}


		std::array<int, 2> computeKernelRange(double sigma, int cutOfFactor, int fieldSize, bool circular);
		std::vector<int> createExtendedIndex(int fieldSize, const std::array<int, 2>& kernelRange);

		std::vector<double> generateNormalVector(int size);

		template <typename T>
		std::vector<std::vector<T>> hebbLearningRule(std::vector<std::vector<T>>& weights, const std::vector<T>& input, const std::vector<T>& targetOutput, double learningRate)
		{
			const int inputSize = input.size();
			const int outputSize = targetOutput.size();

			for (int i = 0; i < inputSize; i++)
				for (int j = 0; j < outputSize; j++)
					weights[i][j] += learningRate * input[i] * targetOutput[j];

			return weights;
		}

		template <typename T>
		std::vector<std::vector<T>> ojaLearningRule(std::vector<std::vector<T>>& weights, const std::vector<T>& input, const std::vector<T>& targetOutput, double learningRate)
		{
			const int inputSize = input.size();
			const int outputSize = targetOutput.size();

			for (int i = 0; i < inputSize; i++)
				for (int j = 0; j < outputSize; j++)
				{
					weights[i][j] = weights[i][j] * (1 - learningRate * std::pow(targetOutput[j], 2)) + learningRate * input[i] * targetOutput[j];
					//weights[i][j] += learningRate * targetOutput[j] * (input[i] - targetOutput[j] * weights[i][j]);
					//weights[i][j] += learningRate * input[i] * (targetOutput[j] - alpha * std::pow(targetOutput[j], 2) * weights[i][j]);
				}
			return weights;
		}

		template <typename T>
		std::vector<std::vector<T>> deltaLearningRuleWidrowHoff(std::vector<std::vector<T>>& weights, const std::vector<T>& input, const std::vector<T>& targetOutput, double learningRate)
		{
			const int inputSize = input.size();
			int outputSize = targetOutput.size();

			// Calculate the activation levels of the fields based on the input values and current weights
			std::vector<T> actualOutput(outputSize, 0.0);
			for (size_t j = 0; j < outputSize; ++j) {
				for (size_t i = 0; i < inputSize; ++i) {
					actualOutput[j] += input[i] * weights[i][j];
				}
			}

			// Calculate the error between the target output and the actual output
			std::vector<T> error(outputSize, 0.0);
			for (size_t j = 0; j < outputSize; ++j) {
				error[j] = targetOutput[j] - actualOutput[j];
			}

			// Update the weights based on the error and current activation levels of the fields
			for (size_t i = 0; i < inputSize; ++i) {
				for (size_t j = 0; j < outputSize; ++j) {
					weights[i][j] += learningRate * error[j] * input[i];
				}
			}

			return weights;
		}

		template <typename T>
		std::vector<std::vector<T>> deltaLearningRuleKroghHertz(std::vector<std::vector<T>>& weights, const std::vector<T>& input, const std::vector<T>& targetOutput, double learningRate)
		{
			const int inputSize = input.size();
			int outputSize = targetOutput.size();

			// Calculate the activation levels of the fields based on the input values and current weights
			std::vector<T> actualOutput(outputSize, 0.0);
			for (size_t j = 0; j < outputSize; ++j) {
				for (size_t i = 0; i < inputSize; ++i) {
					actualOutput[j] += input[i] * weights[i][j];
				}
			}

			// Calculate the error between the target output and the actual output
			std::vector<T> error(outputSize, 0.0);
			for (size_t j = 0; j < outputSize; ++j) {
				error[j] = targetOutput[j] - actualOutput[j];
			}

			// Update the weights based on the error and current activation levels of the fields
			for (size_t i = 0; i < inputSize; ++i) {
				for (size_t j = 0; j < outputSize; ++j) {
					weights[i][j] += learningRate * (error[j] - learningRate * weights[i][j]) * input[i];
				}
			}

			return weights;
		}

		template <typename T>
		bool compareVectors(const std::vector<T>& vec1, const std::vector<T>& vec2, T threshold) {
			if (vec1.size() != vec2.size()) {
				return false; // Vectors are of different sizes, hence not equal
			}

			for (size_t i = 0; i < vec1.size(); ++i) {
				if (std::abs(vec1[i] - vec2[i]) > threshold) {
					return false; // Difference between elements at index i exceeds threshold
				}
			}

			return true; // Vectors are equal within threshold
		}

		template <typename T>
		T calculateVectorSum(const std::vector<T>& vec) {
			T result = 0.0;
			for (T value : vec) {
				result += value;
			}
			return result;
		}

		template <typename T>
		T calculateVectorAvg(const std::vector<T>& vec) {
			if (vec.empty()) {
				return T(); // Return default value if vector is empty
			}

			T sum = T();
			for (const T& value : vec) {
				sum += value;
			}
			return sum / static_cast<T>(vec.size()); // Calculate average
		}

		template <typename T>
		T calculateVectorNorm(const std::vector<T>& vec) {
			T sum_of_squares = std::accumulate(vec.begin(), vec.end(), 0.0,
				[](T a, T b) { return a + b * b; });
			return std::sqrt(sum_of_squares);
		}
	}
}